import cloudscraper
import re
import csv
from bs4 import BeautifulSoup

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù‚Ù†Ø§Øµ
scraper = cloudscraper.create_scraper(browser={'browser': 'chrome','platform': 'android','desktop': False})

def get_links_from_server(page_url):
    """Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø°ÙŠ Ø³Ø£Ù„Øª Ø¹Ù†Ù‡ØŒ ÙŠÙ‚ÙˆÙ… Ø¨Ø³Ø­Ø¨ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø³ÙŠØ±ÙØ±Ø§Øª Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†"""
    links = {"1080p": "", "720p": "", "480p": ""}
    try:
        res = scraper.get(page_url, timeout=15)
        html = res.text
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø³ÙŠØ±ÙØ± Uqload
        uq_match = re.search(r'https?://(?:uqload\.com|uqload\.co)/embed-([a-z0-9]+)', html)
        if uq_match:
            links["1080p"] = f"https://uqload.com/embed-{uq_match.group(1)}.html"
            
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø³ÙŠØ±ÙØ± DoodStream
        dood_match = re.search(r'https?://(?:doodstream\.com|dood\.to|dood\.so)/e/([a-z0-9]+)', html)
        if dood_match:
            links["720p"] = f"https://dood.to/e/{dood_match.group(1)}"
            
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø³ÙŠØ±ÙØ± Upstream
        up_match = re.search(r'https?://(?:upstream\.to|upstream\.org)/embed-([a-z0-9]+)', html)
        if up_match:
            links["480p"] = f"https://upstream.to/embed-{up_match.group(1)}.html"
            
        return links
    except:
        return links

def start_hunting():
    # Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù (ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª)
    target_site = "https://wecima.show/category/%d9%85%d8%b3%d9%84%d8%b3%d9%84%d8%a7%d8%aa-%d8%aa%d8%b1%d9%83%d9%8a%d8%a9/"
    db_file = 'database.csv'
    all_results = []

    print(f"ğŸš€ Ø¬Ø§Ø±ÙŠ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø³Ø­Ø¨ Ù…Ù† Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª...")
    try:
        response = scraper.get(target_site)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒØ±ÙˆØª Ø§Ù„Ø­Ù„Ù‚Ø§Øª (ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„ÙƒÙ„Ø§Ø³ GridItem ØµØ­ÙŠØ­ Ù„Ù„Ù…ÙˆÙ‚Ø¹)
        items = soup.find_all('div', class_='GridItem')

        for item in items[:15]:
            title = item.find('strong').text.strip() if item.find('strong') else "Ø­Ù„Ù‚Ø© ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙØ©"
            page_link = item.find('a')['href']
            
            print(f"ğŸ” ÙØ­Øµ Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª Ù„Ù€: {title}")
            server_links = get_links_from_server(page_link)
            
            all_results.append({
                'name': title,
                'url_1080p': server_links['1080p'],
                'url_720p': server_links['720p'],
                'url_480p': server_links['480p']
            })

        # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        with open(db_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['name', 'url_1080p', 'url_720p', 'url_480p'])
            writer.writeheader()
            writer.writerows(all_results)
        print("âœ¨ Ø§Ù†ØªÙ‡Ù‰! Ø§Ø°Ù‡Ø¨ Ø§Ù„Ø¢Ù† Ù„Ù…Ù„Ù database.csv ÙˆØ³ØªØ¬Ø¯ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª Ø¬Ø§Ù‡Ø²Ø©.")

    except Exception as e:
        print(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {e}")

if __name__ == "__main__":
    start_hunting()
