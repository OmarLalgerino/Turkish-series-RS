import feedparser
import csv
import requests
import re
import cloudscraper
import os

SOURCES = [
    "https://nyaa.si/?page=rss&q=Arabic+1080p",
    "https://nyaa.si/?page=rss&q=Arabic+720p",
    "https://nyaa.si/?page=rss&q=Arabic+480p",
    "https://www.tokyotosho.info/rss.php?filter=1,11&z=Arabic"
]

MAX_ROWS = 10000 

def get_current_db_file():
    i = 0
    while True:
        filename = f'database_{i}.csv' if i > 0 else 'database.csv'
        if not os.path.exists(filename):
            return filename
        with open(filename, 'r', encoding='utf-8') as f:
            row_count = sum(1 for row in f)
        if row_count < MAX_ROWS:
            return filename
        i += 1

def translate_to_arabic_only(text):
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ù…Ù† ÙƒÙ„ Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ù‚Ø¨Ù„ Ø­ÙØ¸Ù‡
    clean_text = re.sub(r'\[.*?\]|\(.*?\)|1080p|720p|480p|HEVC|x264|x265|AAC|Vostfr', '', text).strip()
    try:
        url = f"https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=ar&dt=t&q={requests.utils.quote(clean_text)}"
        res = requests.get(url, timeout=5)
        return res.json()[0][0][0] # Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ ÙÙ‚Ø·
    except:
        return "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"

def get_clean_hash_link(entry):
    if hasattr(entry, 'nyaa_infohash'):
        return f"https://webtor.io/player/embed/{entry.nyaa_infohash}"
    link = getattr(entry, 'link', '')
    hash_match = re.search(r'btih:([a-fA-F0-9]{40})', link)
    if hash_match:
        return f"https://webtor.io/player/embed/{hash_match.group(1).lower()}"
    return None

def start_bot():
    scraper = cloudscraper.create_scraper()
    db_file = get_current_db_file()
    print(f"ðŸš€ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ: {db_file}")

    entries_to_save = []
    for rss_url in SOURCES:
        try:
            resp = scraper.get(rss_url, timeout=15)
            feed = feedparser.parse(resp.text)
            for entry in feed.entries[:30]:
                link = get_clean_hash_link(entry)
                if link:
                    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø§Ø³Ù… Ù„Ù„Ø¹Ø±Ø¨ÙŠ ÙÙˆØ±Ø§Ù‹
                    arabic_title = translate_to_arabic_only(entry.title)
                    
                    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬ÙˆØ¯Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ
                    if "1080p" in entry.title: q = "1080p Ø¹Ø§Ù„ÙŠØ©"
                    elif "720p" in entry.title: q = "720p Ù…ØªÙˆØ³Ø·Ø©"
                    else: q = "480p Ø³Ø±ÙŠØ¹Ø©"
                    
                    # Ù„Ø§Ø­Ø¸ Ù‡Ù†Ø§: Ù„Ø§ ÙŠÙˆØ¬Ø¯ name_en Ø£Ø¨Ø¯Ø§Ù‹
                    entries_to_save.append({
                        'Ø§Ø³Ù…_Ø§Ù„Ø£Ù†Ù…ÙŠ': arabic_title,
                        'Ø§Ù„Ø±Ø§Ø¨Ø·': link,
                        'Ø§Ù„Ø¬ÙˆØ¯Ø©': q
                    })
        except:
            continue

    file_exists = os.path.isfile(db_file)
    with open(db_file, 'a', newline='', encoding='utf-8') as f:
        # Ø±Ø¤ÙˆØ³ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·
        columns = ['Ø§Ø³Ù…_Ø§Ù„Ø£Ù†Ù…ÙŠ', 'Ø§Ù„Ø±Ø§Ø¨Ø·', 'Ø§Ù„Ø¬ÙˆØ¯Ø©']
        writer = csv.DictWriter(f, fieldnames=columns)
        if not file_exists or os.stat(db_file).st_size == 0:
            writer.writeheader()
        writer.writerows(entries_to_save)
    
    print(f"âœ… ØªÙ…! Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø¢Ù† Ø¹Ø±Ø¨ÙŠ Ø®Ø§Ù„Øµ ÙˆØ¨Ø¯ÙˆÙ† Ø£ÙŠ Ø®Ø§Ù†Ø§Øª Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©.")

if __name__ == "__main__":
    start_bot()
